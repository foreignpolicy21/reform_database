---
title: "reform_db_cleaning"
output: html_document
date: "2023-10-12"
---

```{r}
library(tidyverse)
library(pdftools)
library(httr)
```

# Read source CSV and clean up the columns a bit
```{r}
sources <- read_csv("sources.csv")[-1]

# filter only the relevant fields
sources <- sources %>% 
  select(`Title`, `Item Type`, Author, `Publication Title`, `File Attachments`, Url, Date, Publisher, Type, `Manual Tags`)

# create Source Type, Document Type, and Organization or Publisher fields
sources <- sources %>%
  mutate(`Organization or Publisher` = ifelse(`Item Type` == "magazineArticle", `Publication Title`, Publisher), # combine Publication Title and Publisher to Organization or Publisher
         `Source Type` = Type) %>% # add a Source Type based on Report Type parameter from Zotero
  # rename document types
  mutate(`Document Type` = str_replace(`Item Type`, "report", "Report"),
         `Document Type` = str_replace(`Document Type`, "magazineArticle", "Op-Ed"),
         `Document Type` = str_replace(`Document Type`, "bookSection", "Book Section"),
         `Document Type` = str_replace(`Document Type`, "book", "Book"),
         `Document Type` = str_replace(`Document Type`, "document", "Misc Document")) %>%
  select(-c(`Publication Title`, Publisher, `Item Type`, Type))

# noncomprehensive lists of organizations in source type categorizing for assigning Source Type to various sources that do not have them via Zotero
magazines <- c("American Purpose", "The Foreign Service Journal", "Just Security", "Foreign Policy", "Democracy Journal", "Responsible Statecraft", "Foreign Affairs")
thinktanks <- c("The Brookings Institution", "The Aspen Institute","Hoover Institution Press, Stanford University", "RAND Corporation")
govt_offices <- c("U.S. Department of State: Office of Inspector General", "U.S. Department of State")
academic <- c("RAND Graduate School", "Institute for the Study of Diplomacy", "Penn State University Press")
nonprofits <- c("Foreign Policy Association", "American Academy of Diplomacy")

# hand-label misc documents' Source Types
sources <- sources %>% 
  mutate(`Source Type` = ifelse(`Organization or Publisher` %in% govt_offices, "Government Office", `Source Type`),
         `Source Type` = ifelse(`Organization or Publisher` %in% thinktanks, "Think Tank", `Source Type`),
         `Source Type` = ifelse(`Organization or Publisher` %in% nonprofits, "Nonprofit", `Source Type`),
         `Source Type` = ifelse(`Organization or Publisher` %in% academic, "Academic", `Source Type`),
         `Source Type` = ifelse(`Organization or Publisher` %in% magazines, "News Media",`Source Type`))
      

# create Contains Recommendations field
sources <- sources %>% 
  mutate(`Contains Recommendations` = ifelse(grepl("rec map", `Manual Tags`), "TRUE", "FALSE")) %>%
  select(-`Manual Tags`)
```


# Extract full text from PDFs
```{r}
sources <- sources %>% 
  mutate(`Path to PDF` = "",
         `Document Text` = "")

fpaths_cleaned <- c()

# remove spaces ONLY from start and end of string
remove_spaces <- function(string) {
  string <- sub("^\\s+", "", string)
  string <- sub("\\s+$", "", string)
  string
}

# check if filepath ends in "pdf"
is_pdf <- function(string) {
  substr(string,nchar(string)-2,nchar(string)) == "pdf"
}

for (i in 1:nrow(sources)) {
  fpath <- sources[i,]$`File Attachments`
  
  # split by semicolon
  fpath_cands <- strsplit(fpath, ";")
  
  # remove whitespace on front & back
  fpath_cands <- lapply(fpath_cands, remove_spaces)
  
  # return the first one ending with "pdf"
  pdf_path_index <- head(which(sapply(fpath_cands, is_pdf)), 1)
  new_fpath <- ifelse(length(pdf_path_index) > 0, fpath_cands[[1]][pdf_path_index], "")
  
  if(nchar(new_fpath) != 0) {
    sources[i,]$`Path to PDF` <- new_fpath
    
    # get the full text
    filetext <- pdf_text(new_fpath)
    filetext_cat <- paste(filetext, collapse = " ")
    filetext_cleaned <- gsub("\n", "", filetext_cat)
    
    sources[i,]$`Document Text` <- filetext_cleaned
  }
}
```


# Add Summaries
First, manually upload CSV to Relevance to summarize (might be worth doing this thru the API eventually).
```{r}
write_csv(sources, "sources_without_summaries.csv")
```


# Clean summarized DF
Run the following code only after summarizing with Relevance:

```{r}
sources <- read_csv("sources_with_summaries.csv")

# select the right rows
sources <- sources %>% 
  select(-c(`File-Attachments`, `Path-to-PDF`, `Document-Text`)) %>%
  rename( `Document Type` = `Document-Type`,
          `Organization or Publisher` = `Organization-or-Publisher`,
          `Source Type` = `Source-Type`,
          `Summary (from GPT)` = answer,
          `Contains Recommendations` = `Contains-Recommendations`,
          URL = Url)

# remove bad GPT summary columns
sources <- sources %>% 
  mutate(`Summary (from GPT)` = ifelse(str_detect(`Summary (from GPT)`, "Discusses the document, which is titled"), "", `Summary (from GPT)`)) %>%
  mutate(`Summary (from GPT)` = ifelse(str_detect(`Summary (from GPT)`, "NA"), "", `Summary (from GPT)`)) %>%
  mutate(`Summary (from GPT)` = ifelse(is.na(`Summary (from GPT)`), "", `Summary (from GPT)`))
```

```{r}
colnames(sources)
```



# Write CSV for upload to Notion
```{r}
# TODO:
# Make it so everything goes into Notion with the right type, factors broken down correctly, etc.
# turn `Organization or Publisher` and other columns into multi-select type beat (factor?)

# remove unnecessary columns

write_csv(sources, "sources_updated.csv")
```
